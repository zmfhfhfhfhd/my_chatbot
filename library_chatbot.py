# -*- coding: utf-8 -*-
import os
import sys
import streamlit as st

# -------------------------------------------------------------------
# âœ… sqlite3 í˜¸í™˜ (Streamlit Cloud ë“± ì¼ë¶€ í™˜ê²½ì—ì„œ Chromaê°€ sqlite3 ë¹Œë“œ ì´ìŠˆë¥¼ ì¼ìœ¼í‚¬ ë•Œ ëŒ€ì‘)
#    - ë°˜ë“œì‹œ Chroma/ChromaDB import "ì´ì „"ì— ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
# -------------------------------------------------------------------
try:
    __import__("pysqlite3")
    sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")
except Exception:
    # pysqlite3ê°€ ì—†ê±°ë‚˜ êµì²´ê°€ ë¶ˆí•„ìš”í•œ í™˜ê²½ì´ë©´ ê·¸ëŒ€ë¡œ ì§„í–‰
    pass

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_chroma import Chroma

# -------------------------------------------------------------------
# âœ… API Key (Streamlit secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œë§Œ ì½ê¸°)
# -------------------------------------------------------------------
if not os.getenv("OPENAI_API_KEY"):
    # secrets.tomlì— OPENAI_API_KEYê°€ ìˆëŠ” ê²½ìš° ìë™ ì£¼ì…
    if "OPENAI_API_KEY" in st.secrets:
        os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# -------------------------------------------------------------------
# âœ… ìºì‹œ í•¨ìˆ˜ë“¤
# -------------------------------------------------------------------
@st.cache_resource(show_spinner=False)
def load_and_split_pdf(file_path: str):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

@st.cache_resource(show_spinner=False)
def build_or_load_vectorstore(_docs, persist_directory: str = "./chroma_db"):
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ê¸°ì¡´ DBê°€ ìˆìœ¼ë©´ ë¡œë“œ ì‹œë„
    if os.path.isdir(persist_directory) and any(os.scandir(persist_directory)):
        try:
            return Chroma(persist_directory=persist_directory, embedding_function=embeddings)
        except Exception:
            # ì†ìƒ/ë²„ì „ë¶ˆì¼ì¹˜ ë“±ì˜ ì´ìœ ë¡œ ë¡œë“œ ì‹¤íŒ¨í•˜ë©´ ìƒˆë¡œ ìƒì„±
            pass

    # ìƒˆë¡œ ìƒì„±
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    split_docs = text_splitter.split_documents(_docs)
    return Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=persist_directory,
    )

@st.cache_resource(show_spinner=False)
def initialize_chain(selected_model: str, pdf_path: str):
    pages = load_and_split_pdf(pdf_path)
    vectorstore = build_or_load_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question which might reference context "
        "in the chat history, formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, just reformulate it if "
        "needed and otherwise return it as is."
    )
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # QA í”„ë¡¬í”„íŠ¸
    qa_system_prompt = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer the question. "
        "If you don't know the answer, just say that you don't know. "
        "Keep the answer perfect. please use emoji with the answer. "
        "ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\n\n"
        "{context}"
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    llm = ChatOpenAI(model=selected_model)
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# -------------------------------------------------------------------
# âœ… Streamlit UI
# -------------------------------------------------------------------
st.set_page_config(page_title="ì˜í™” ì¶”ì²œ ì±—ë´‡", page_icon="ğŸ“š")
st.header("ì˜í™” ì¶”ì²œ ì±—ë´‡")

# ëª¨ë¸ ì„ íƒ
option = st.selectbox("Select GPT Model", ("gpt-4o-mini", "gpt-3.5-turbo-0125"))

# PDF ì„ íƒ: (1) ë ˆí¬ì— ìˆëŠ” ê¸°ë³¸ PDF ê²½ë¡œ, (2) ì—…ë¡œë“œ
DEFAULT_PDF = "ì œëª© ì—†ëŠ” ë¬¸ì„œ.pdf"

uploaded = st.file_uploader("PDFë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜, ê¸°ë³¸ PDFë¡œ ì‹¤í–‰í•˜ì„¸ìš”.", type=["pdf"])
pdf_path = None

if uploaded is not None:
    # ì—…ë¡œë“œ íŒŒì¼ì€ ì„ì‹œë¡œ ì €ì¥ í›„ ì‚¬ìš©
    tmp_dir = Path(".streamlit_tmp")
    tmp_dir.mkdir(exist_ok=True)
    pdf_path = str(tmp_dir / uploaded.name)
    with open(pdf_path, "wb") as f:
        f.write(uploaded.getbuffer())
else:
    # ê¸°ë³¸ íŒŒì¼ì´ ë ˆí¬ì— í¬í•¨ë¼ ìˆë‹¤ë©´ ìƒëŒ€ê²½ë¡œë¡œ ì ‘ê·¼
    if os.path.exists(DEFAULT_PDF):
        pdf_path = DEFAULT_PDF

if not pdf_path:
    st.info("ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì‹œê±°ë‚˜, ë ˆí¬ì— ê¸°ë³¸ PDF íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

rag_chain = initialize_chain(option, pdf_path)
chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

# ê¸°ì¡´ ëŒ€í™” ë Œë”ë§
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

# ì…ë ¥
if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.chat_message("human").write(prompt_message)
    with st.chat_message("ai"):
        with st.spinner("Thinking..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke({"input": prompt_message}, config)
            answer = response.get("answer", "")
            st.write(answer)

            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                for doc in response.get("context", []):
                    src = doc.metadata.get("source", "source")
                    st.markdown(src, help=doc.page_content)



